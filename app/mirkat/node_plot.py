from altair import Chart
from langchain_core.messages import ( 
    AIMessage
    )
from google.genai import types
from google.genai import Client
from dotenv import load_dotenv
from app.mirkat.plot_functions import PlotFunctons
import base64
import io, os
from app.mirkat.global_variables import SQL_QUERIES
from app.mirkat.node_constructor import node
import json


load_dotenv()

# Get the API key
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

class PlotNode(node):
    def __init__(self, llm=None, instructions=None, functions=None,  welcome=None):
        super().__init__(llm, instructions, functions, welcome, logging_key="Plot node.- ")
        self.client = Client(api_key=GOOGLE_API_KEY)
        self.schema = {
            "type": "object",
            "properties": {
                "caption": {"type": "string"},
                "code": {"type": "string"},
                "notes": {"type": "string"}
            },
            "required": ["caption", "code"]
        }
        self.set_model()


    def set_model(self):
        config_with_code = types.GenerateContentConfig(
            #tools=[types.Tool(code_execution=types.ToolCodeExecution())],
            temperature=0.0,
            response_schema= self.schema,
            response_mime_type="application/json"
        )
        self.model = self.client.chats.create(model=self.llm, config=config_with_code)

    def run_model(self, messages):
        """Run the model with the given messages."""
        #print(f"--- Message going to the llm_master: {messages}---")
        self.log_message(f"Message going to the node: {messages}")
        response_plot = self.model.send_message(messages)
        return response_plot
    
    def handle_response(self, response_plot):
        plotting_tools_instance = PlotFunctons('', response_plot)
        plotting_tools_instance.handle_response()

    def run_code_plot(self, code):
        """
        This function will take the code generated by the model and plot it.
        """
        loc = {}
        exec(code, globals(), loc)
        return_workaround = loc['figure'] if 'figure' in loc else None
        return return_workaround

    def extract_response(self, response_plot):
        """
        This function will take the answer and proceed to convert to json
        """
        response_json = {}
        if isinstance(response_plot, json):
            response_json = response_plot
        if isinstance(response_plot, str):
            response_json = json.loads(response_plot)
        return response_json

    def get_node(self, state):
        messages = state['request']
        self.log_message(f"Messages received in PlotNode: {messages}")
        queries = SQL_QUERIES # state['table']

        response_plot = self.run_model(str(queries) + self.instructions + messages.content)
        response_json = self.extract_response(response_plot=response_plot)
        plot = self.run_code_plot(response_json.code)
        
        answer = response_json.caption

        answer_b = answer
        if plot:
            buf = io.BytesIO()
            if not isinstance(plot, Chart):
                plot.savefig(buf, format='png') # Or another format like 'jpeg'
            elif isinstance(plot, Chart):
                plot.save(buf, format='json')
            buf.seek(0)
            image_base64 = base64.b64encode(buf.read()).decode('utf-8')
            buf.close()
            # response_plot.candidates[0].content.parts[0].text =  f"binary_image: {image_base64}"
            answer_b = answer + f"binary_image: {image_base64}"
        history = state.get("history", [])
        return {**state,
                "messages": AIMessage(content=""),
                "answer": answer_b,
                "request": AIMessage(content=answer_b),
                "answer_source": 'PlotNode',
                "history": history + [messages], # Update history with the new message
            }
    
    
